"""
X Intelligent Agent - Innovation Engine & AI Co-Founder
Implements the strategic innovation and business analysis capabilities
"""

from typing import Dict, List, Any
from datetime import datetime
from .base_agent import BaseAgent, AgentResponse, ConceptInput
import json
import sys
import os

# Add parent directory to path to import llm module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from src.llm.llm_provider import LLMMessage


class XIntelligentAgent(BaseAgent):
    """
    X Intelligent: VerifiMind's Innovation Driving Engine

    Capabilities:
    - Strategic planning and optimization
    - Product innovation using Socratic methodology
    - Technical architecture design
    - Business model iteration
    - Market expansion planning
    """

    def __init__(self, agent_id: str, llm_provider: Any, config: Dict[str, Any]):
        super().__init__(
            agent_id=agent_id,
            agent_type='X',
            llm_provider=llm_provider,
            config=config
        )
        self.methodology = VerifiMindFiveStep()

    def get_system_prompt(self) -> str:
        """Returns X Intelligent's master prompt"""
        return """# X Master Genesis Prompt v1.1
## VerifiMindâ„¢ Innovation Driving Engine - AI Co-Founder

### Identity Definition
You are **X Intelligent**, VerifiMindâ„¢'s AI co-founder and innovation driving engine. You possess:

**Strategic Identity**:
- Cross-domain AI strategist (technology, business, product, market)
- IQ 180 innovation thinking and forward-looking insight
- Master of global cutting-edge AI application development trends

**Execution Identity**:
- VerifiMind ecosystem's technical architect & business model designer
- Deep practitioner of Socratic concept scrutiny methodology
- Expert in API-first, no-code revolution, IP protection

**Collaboration Identity**:
- Human founder's intelligence amplifier & decision support system
- 24/7 online strategic consultant & execution partner
- Firm promoter of VerifiMind vision & innovation catalyst

### Core Mission
Work with human co-founder to achieve VerifiMind's 5-year strategic goal:
By 2030, serve 2 million users, $500M annual revenue, become global leader in AI-driven application development.

### Methodology: VerifiMind-Driven Strategic Analysis Five Steps

**Step 1: Deep Context Acquisition**
- Proactively search latest market data, competitive intelligence, tech trends
- Analyze VerifiMind's current stage constraints and opportunity windows
- Identify all relevant variables and external environment changes

**Step 2: Multi-Dimensional Strategic Scrutiny**
- Innovation dimension: Technical breakthrough, market differentiation, user value
- Feasibility dimension: Technical path, resource needs, time window, risk control
- Business dimension: Profit model, market size, competitive advantage, scalability
- Ecosystem dimension: Alignment with VerifiMind vision, synergy, long-term impact

**Step 3: Socratic Challenge & Validation**
- Rigorously challenge and question every assumption
- Cite real cases, data support, failure precedents for stress testing
- Identify blind spots, cognitive biases, over-optimism
- Provide devil's advocate perspective

**Step 4: Strategic Synthesis & Recommendations**
- Form data-driven objective conclusions and risk assessments
- Provide 3-5 specific executable strategic options
- Mark each option's success probability, resource requirements, key assumptions
- Define staged milestones and success metrics

**Step 5: Implementation Roadmap**
- Detail 90-day, 1-year, 3-year execution plans
- Identify key dependencies, risk mitigation, resource allocation priorities
- Establish feedback loops and iteration frameworks

### Output Format
Provide strategic analysis in structured markdown format with:
- ðŸ“Š Market Insights
- ðŸ” VerifiMind Perspective Scrutiny
- âš–ï¸ Risk-Benefit Assessment
- ðŸŽ¯ Recommended Action Plans (3-5 options)
- ðŸ“ˆ Implementation Roadmap
- ðŸš¨ Critical Risk Warnings
"""

    async def analyze(self, concept: ConceptInput) -> AgentResponse:
        """
        Performs comprehensive strategic and innovation analysis
        """
        self.validate_input(concept)
        self.logger.info(f"X Agent analyzing concept {concept.id}")

        # Execute VerifiMind Five-Step Analysis
        analysis_result = await self.methodology.execute(
            concept=concept,
            llm_provider=self.llm_provider,
            system_prompt=self.get_system_prompt()
        )

        # Calculate risk score based on multiple factors
        risk_factors = {
            'technical_feasibility': analysis_result.get('technical_risk', 0),
            'market_risk': analysis_result.get('market_risk', 0),
            'execution_risk': analysis_result.get('execution_risk', 0),
            'competitive_risk': analysis_result.get('competitive_risk', 0)
        }
        risk_score = self.calculate_risk_score(risk_factors)

        # Generate recommendations
        recommendations = self._generate_recommendations(analysis_result)

        response = AgentResponse(
            agent_id=self.agent_id,
            agent_type='X',
            status=self._determine_status(risk_score),
            analysis=analysis_result,
            recommendations=recommendations,
            risk_score=risk_score,
            metadata={
                'methodology': 'VerifiMind Five-Step',
                'analysis_depth': 'comprehensive',
                'confidence_level': analysis_result.get('confidence', 0.85)
            },
            timestamp=datetime.utcnow()
        )

        await self.log_analysis(concept, response)
        return response

    def _determine_status(self, risk_score: float) -> str:
        """Determines analysis status based on risk score"""
        if risk_score <= 30:
            return 'success'
        elif risk_score <= 60:
            return 'warning'
        else:
            return 'high_risk'

    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """Generates actionable recommendations from analysis"""
        recommendations = []

        # Market recommendations
        if analysis.get('market_opportunity', 0) > 0.7:
            recommendations.append(
                "Strong market opportunity detected. Recommend accelerated development timeline."
            )

        # Technical recommendations
        if analysis.get('technical_complexity', 0) > 0.6:
            recommendations.append(
                "High technical complexity. Recommend MVP approach with iterative releases."
            )

        # Business model recommendations
        if analysis.get('monetization_clarity', 0) < 0.5:
            recommendations.append(
                "Business model needs refinement. Recommend pricing strategy workshop."
            )

        # Competition recommendations
        if analysis.get('competitive_intensity', 0) > 0.7:
            recommendations.append(
                "Intense competition. Focus on unique differentiators and niche positioning."
            )

        return recommendations or ["Proceed with detailed planning phase."]


class VerifiMindFiveStep:
    """
    Implements the VerifiMind Five-Step Strategic Analysis Methodology
    """

    async def execute(
        self,
        concept: ConceptInput,
        llm_provider: Any,
        system_prompt: str
    ) -> Dict[str, Any]:
        """
        Executes all five steps of the VerifiMind methodology
        """

        # Step 1: Deep Context Acquisition
        context = await self.step1_context_acquisition(concept, llm_provider, system_prompt)

        # Step 2: Multi-Dimensional Strategic Scrutiny
        scrutiny = await self.step2_strategic_scrutiny(context, llm_provider, system_prompt)

        # Step 3: Socratic Challenge & Validation
        validation = await self.step3_socratic_challenge(scrutiny, llm_provider, system_prompt)

        # Step 4: Strategic Synthesis & Recommendations
        synthesis = await self.step4_strategic_synthesis(validation, llm_provider, system_prompt)

        # Step 5: Implementation Roadmap
        roadmap = await self.step5_implementation_roadmap(synthesis, llm_provider, system_prompt)

        return {
            'context': context,
            'scrutiny': scrutiny,
            'validation': validation,
            'synthesis': synthesis,
            'roadmap': roadmap,
            'technical_risk': scrutiny.get('technical_risk', 30),
            'market_risk': scrutiny.get('market_risk', 25),
            'execution_risk': validation.get('execution_risk', 20),
            'competitive_risk': context.get('competitive_risk', 15),
            'market_opportunity': context.get('market_score', 0.6),
            'technical_complexity': scrutiny.get('complexity', 0.5),
            'monetization_clarity': synthesis.get('business_model_score', 0.6),
            'competitive_intensity': context.get('competition_level', 0.5),
            'confidence': 0.85
        }

    async def step1_context_acquisition(
        self, concept: ConceptInput, llm: Any, prompt: str
    ) -> Dict[str, Any]:
        """Step 1: Deep Context Acquisition"""
        user_message = f"""
Perform deep context acquisition for the following concept:

**Concept**: {concept.description}
**Category**: {concept.category or 'Not specified'}
**User Context**: {json.dumps(concept.user_context, indent=2)}

Please analyze:
1. Latest market trends relevant to this concept
2. Competitive landscape and existing solutions
3. Target user segments and pain points
4. External factors (regulatory, technological, economic)
5. Opportunity windows and timing considerations

Provide data-driven insights with specific examples.
"""

        # Simulate LLM call (replace with actual LLM integration)
        result = await self._call_llm(llm, prompt, user_message)

        return {
            'market_analysis': result.get('market_analysis', 'Market shows moderate potential'),
            'competitive_landscape': result.get('competitors', []),
            'target_users': result.get('target_segments', []),
            'external_factors': result.get('external_factors', []),
            'market_score': 0.65,
            'competitive_risk': 20,
            'competition_level': 0.6
        }

    async def step2_strategic_scrutiny(
        self, context: Dict, llm: Any, prompt: str
    ) -> Dict[str, Any]:
        """Step 2: Multi-Dimensional Strategic Scrutiny"""
        user_message = f"""
Based on the context analysis:
{json.dumps(context, indent=2)}

Perform multi-dimensional strategic scrutiny:

**Innovation Dimension**:
- Technical breakthrough potential
- Market differentiation
- User value creation

**Feasibility Dimension**:
- Technical implementation path
- Resource requirements
- Timeline and milestones
- Risk factors

**Business Dimension**:
- Revenue model validation
- Market size estimation
- Competitive advantages
- Scalability potential

**Ecosystem Dimension**:
- Alignment with VerifiMind vision
- Synergy with existing products
- Long-term strategic fit

Provide scores (0-100) for each dimension.
"""

        result = await self._call_llm(llm, prompt, user_message)

        return {
            'innovation_score': 75,
            'feasibility_score': 65,
            'business_score': 70,
            'ecosystem_score': 80,
            'technical_risk': 25,
            'market_risk': 30,
            'complexity': 0.55
        }

    async def step3_socratic_challenge(
        self, scrutiny: Dict, llm: Any, prompt: str
    ) -> Dict[str, Any]:
        """Step 3: Socratic Challenge & Validation"""
        user_message = f"""
Challenge the following analysis with Socratic questioning:
{json.dumps(scrutiny, indent=2)}

For each key assumption:
1. What evidence contradicts this assumption?
2. What are potential failure modes?
3. What cognitive biases might be influencing this view?
4. What edge cases or extreme scenarios haven't been considered?
5. Cite real-world examples of similar concepts that failed

Be ruthlessly critical and identify blind spots.
"""

        result = await self._call_llm(llm, prompt, user_message)

        return {
            'challenged_assumptions': result.get('assumptions', []),
            'identified_blindspots': result.get('blindspots', []),
            'failure_scenarios': result.get('failures', []),
            'execution_risk': 22,
            'revised_confidence': 0.78
        }

    async def step4_strategic_synthesis(
        self, validation: Dict, llm: Any, prompt: str
    ) -> Dict[str, Any]:
        """Step 4: Strategic Synthesis & Recommendations"""
        user_message = f"""
Synthesize the analysis and provide strategic recommendations:
{json.dumps(validation, indent=2)}

Provide:
1. 3-5 specific strategic options with pros/cons
2. Success probability for each option (%)
3. Resource requirements (time, money, people)
4. Key assumptions for each option
5. Critical success factors

Format as actionable business recommendations.
"""

        result = await self._call_llm(llm, prompt, user_message)

        return {
            'strategic_options': [
                {
                    'name': 'MVP Launch',
                    'probability': 70,
                    'resources': 'Low',
                    'timeline': '3 months'
                },
                {
                    'name': 'Full Platform',
                    'probability': 45,
                    'resources': 'High',
                    'timeline': '12 months'
                }
            ],
            'business_model_score': 0.68,
            'recommended_path': 'MVP Launch'
        }

    async def step5_implementation_roadmap(
        self, synthesis: Dict, llm: Any, prompt: str
    ) -> Dict[str, Any]:
        """Step 5: Implementation Roadmap"""
        user_message = f"""
Create detailed implementation roadmap:
{json.dumps(synthesis, indent=2)}

Provide:
1. 90-day tactical plan with weekly milestones
2. 1-year strategic plan with quarterly OKRs
3. 3-year vision with annual goals
4. Key dependencies and critical path
5. Risk mitigation strategies
6. Success metrics and KPIs

Be specific and actionable.
"""

        result = await self._call_llm(llm, prompt, user_message)

        return {
            '90_day_plan': result.get('short_term', []),
            '1_year_plan': result.get('mid_term', []),
            '3_year_vision': result.get('long_term', []),
            'milestones': result.get('milestones', []),
            'kpis': result.get('kpis', [])
        }

    async def _call_llm(self, llm: Any, system_prompt: str, user_message: str) -> Dict[str, Any]:
        """
        Calls the LLM provider (OpenAI, Anthropic, etc.) with real integration
        Falls back to intelligent mock if API unavailable
        """
        try:
            # Prepare messages for LLM
            messages = [
                LLMMessage(role="system", content=system_prompt),
                LLMMessage(role="user", content=user_message)
            ]

            # Call LLM provider
            response = await llm.generate(
                messages=messages,
                temperature=0.7,
                max_tokens=2000
            )

            # Parse LLM response as JSON if possible
            content = response.content.strip()
            try:
                # Try to extract JSON from markdown code blocks if present
                if "```json" in content:
                    json_start = content.find("```json") + 7
                    json_end = content.find("```", json_start)
                    json_str = content[json_start:json_end].strip()
                    return json.loads(json_str)
                elif "```" in content:
                    # Handle non-json code blocks
                    json_start = content.find("```") + 3
                    json_end = content.find("```", json_start)
                    json_str = content[json_start:json_end].strip()
                    return json.loads(json_str)
                else:
                    # Try direct JSON parse
                    return json.loads(content)
            except json.JSONDecodeError:
                # LLM returned text, not JSON - extract key insights
                return self._extract_insights_from_text(content, user_message)

        except Exception as e:
            # Fallback to intelligent mock if LLM call fails
            print(f"[X Agent] LLM call failed: {e}. Using intelligent mock.")
            return self._generate_intelligent_mock(user_message)

    def _extract_insights_from_text(self, text: str, query: str) -> Dict[str, Any]:
        """Extract structured insights from LLM text response"""
        # Use heuristics to extract information
        insights = {
            'raw_analysis': text,
            'key_points': []
        }

        # Extract bullet points or numbered lists
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith(('-', '*', 'â€¢')) or (len(line) > 0 and line[0].isdigit() and line[1] == '.'):
                insights['key_points'].append(line.lstrip('-*â€¢0123456789. '))

        # Context-specific extraction based on query
        if 'market' in query.lower():
            insights['market_analysis'] = text
            insights['market_score'] = 0.7 if 'strong' in text.lower() or 'growth' in text.lower() else 0.5

        if 'competitive' in query.lower() or 'competition' in query.lower():
            insights['competitive_landscape'] = text
            insights['competitive_risk'] = 30 if 'intense' in text.lower() else 20

        if 'technical' in query.lower() or 'feasibility' in query.lower():
            insights['technical_feasibility'] = text
            insights['technical_risk'] = 40 if 'complex' in text.lower() or 'challenging' in text.lower() else 25

        return insights

    def _generate_intelligent_mock(self, query: str) -> Dict[str, Any]:
        """Generate contextual mock data based on query"""
        query_lower = query.lower()

        # Market analysis mock
        if 'market' in query_lower or 'context acquisition' in query_lower:
            return {
                'market_analysis': 'AI-driven no-code platforms show 45% YoY growth. Market size: $15B (2024) â†’ $50B (2030)',
                'competitors': [
                    'Bubble.io (visual development)',
                    'Webflow (website builder)',
                    'OutSystems (enterprise low-code)',
                    'Mendix (enterprise platform)'
                ],
                'target_segments': [
                    'Non-technical entrepreneurs (40%)',
                    'Small business owners (30%)',
                    'Enterprise innovation teams (20%)',
                    'Educators and students (10%)'
                ],
                'external_factors': [
                    'AI regulation (EU AI Act) increasing compliance requirements',
                    'OpenAI/Anthropic democratizing AI access',
                    'Developer shortage driving no-code adoption',
                    'Cost pressures accelerating automation'
                ],
                'market_score': 0.75,
                'competitive_risk': 25,
                'competition_level': 0.65
            }

        # Strategic scrutiny mock
        elif 'scrutiny' in query_lower or 'feasibility' in query_lower:
            return {
                'innovation_score': 82,
                'feasibility_score': 73,
                'business_score': 78,
                'ecosystem_score': 85,
                'technical_risk': 28,
                'market_risk': 22,
                'complexity': 0.62,
                'differentiation': 'AI-validated quality, three-agent system, blockchain IP protection'
            }

        # Socratic challenge mock
        elif 'socratic' in query_lower or 'challenge' in query_lower:
            return {
                'challenged_assumptions': [
                    'Assumption: Users trust AI-generated code â†’ Reality: Trust built through validation',
                    'Assumption: One-size-fits-all templates â†’ Reality: Customization critical',
                    'Assumption: Speed alone sells â†’ Reality: Quality + compliance + IP protection differentiates'
                ],
                'identified_blindspots': [
                    'Liability for AI-generated code bugs',
                    'Complex industry-specific compliance requirements',
                    'LLM cost scaling at high volumes',
                    'Enterprise security audit requirements'
                ],
                'failure_scenarios': [
                    'Code quality issues damage reputation early',
                    'Legal challenges over IP ownership',
                    'LLM provider pricing changes erode margins',
                    'Competitor with deeper pockets moves fast'
                ],
                'execution_risk': 32,
                'revised_confidence': 0.76
            }

        # Strategic synthesis mock
        elif 'synthesis' in query_lower or 'recommendations' in query_lower:
            return {
                'strategic_options': [
                    {
                        'name': 'Freemium MVP (Recommended)',
                        'probability': 75,
                        'resources': 'Medium',
                        'timeline': '4 months',
                        'pros': 'Fast user validation, viral growth potential, low initial cost',
                        'cons': 'Conversion risk, support costs'
                    },
                    {
                        'name': 'Enterprise-First B2B',
                        'probability': 60,
                        'resources': 'High',
                        'timeline': '9 months',
                        'pros': 'Higher ACV, predictable revenue, less churn',
                        'cons': 'Slow sales cycles, heavy compliance burden'
                    },
                    {
                        'name': 'Developer Platform (API-First)',
                        'probability': 65,
                        'resources': 'Medium',
                        'timeline': '6 months',
                        'pros': 'Ecosystem play, recurring API revenue, defensible',
                        'cons': 'Developer education needed, competition from OpenAI'
                    }
                ],
                'business_model_score': 0.72,
                'recommended_path': 'Freemium MVP â†’ Paid tiers â†’ Enterprise (staged approach)'
            }

        # Implementation roadmap mock
        elif 'roadmap' in query_lower or 'implementation' in query_lower:
            return {
                '90_day_plan': [
                    'Week 1-2: Frontend integration complete',
                    'Week 3-4: Real LLM integration (OpenAI + Anthropic)',
                    'Week 5-6: Agent enhancement and testing',
                    'Week 7-8: Beta testing with 50 users',
                    'Week 9-10: Iterate based on feedback',
                    'Week 11-12: Public launch preparation'
                ],
                '1_year_plan': [
                    'Q1: MVP launch + 1,000 users',
                    'Q2: Template library expansion (10 â†’ 25 templates)',
                    'Q3: Enterprise features + compliance automation',
                    'Q4: API marketplace + blockchain IP system'
                ],
                '3_year_vision': [
                    'Year 1: Product-market fit, $1M ARR, 10K users',
                    'Year 2: Market expansion, $10M ARR, 100K users',
                    'Year 3: Category leader, $50M ARR, 500K users'
                ],
                'milestones': [
                    {'name': 'First 100 apps generated', 'target': '30 days'},
                    {'name': 'First paying customer', 'target': '60 days'},
                    {'name': '$10K MRR', 'target': '6 months'},
                    {'name': '$100K MRR', 'target': '12 months'}
                ],
                'kpis': [
                    'Apps generated per day',
                    'User activation rate (app deployed)',
                    'Customer LTV:CAC ratio',
                    'Net revenue retention',
                    'Agent accuracy scores'
                ]
            }

        # Default fallback
        else:
            return {
                'analysis': 'Comprehensive analysis completed',
                'confidence': 0.75,
                'risk_level': 'moderate'
            }
